[{"categories":["Natural Language Processing"],"content":"Implementing a number of deep learning approaches to generate headlines from news articles","date":"2020-05-06","objectID":"/headline-writer/","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"This repository contains implementations of Sequence-to-sequence (Seq2Seq) neural networks for abstractive text summarization. We implement Attention mechanism, Teacher Forcing algorithm, and Pointer-Generator Network (inspired by Get To The Point: Summarization with Pointer-Generator Networks) in our experiment to improve our baseline models. This work was done in collaboration with Henglin Wu, Ruilin Zhao, and Chenyuan Li. [paper] [code] Watch our video presentation below: ","date":"2020-05-06","objectID":"/headline-writer/:0:0","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"Introduction Sequence-to-sequence (Seq2Seq) neural networks have been proved to be effective in tasks that involve transforming text from one form to another (e.g. machine translation and speech recognition). They can also be used to generate summarization from text input. In this project, we compare summarization results of a non-deep learning method and results of different implementations of the Seq2Seq network. The deep learning implementations include using bi-directional long short-term memory (BiLSTM), additive attention, and teacher forcing in the Seq2Seq network. More advanced approaches using pointer-generator and coverage are also applied to improve the summarization results. ","date":"2020-05-06","objectID":"/headline-writer/:0:1","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"Related Work Our project is motivated by and built upon previous research in machine translation. Sutskever et al (2014) proposed the Seq2Seq structure, in which a LSTM layer is used to encode the input text to a vector of fixed dimensionality and another LSTM layer is used to decode the target sequence from the vector. Expanding on the structure of Seq2Seq, Bahdanau et al (2014) showed that adding an additive attention extension may help the network search a set of positions where the most relevant information is concentrated and predicts the target word based on the context vectors associated with these source positions. Vaswani et al (2017) built on the self-attention model and created multi-attention, which allows the model to jointly attend to information from different representation subspaces at different positions. Going back to the early days of recurrent neural networks (RNNs), a method called teacher forcing was used to help RNNs converge faster. When the predictions are unsatisfactory in the beginning and the hidden states would be updated with a sequence of wrong predictions, the errors would accumulate. Teacher forcing was shown to mitigate this problem Williams et al (1989). In 2017, See et al (2017) applied a new structure using a pointer-generator network that can copy words from the source text via pointing and coverage to keep track of the content that has been summarized to avoid repetition. Our project adopts these breakthrough networks and features step by step and shows the improvements on our specific abstractive summarization task. ","date":"2020-05-06","objectID":"/headline-writer/:0:2","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"Dataset We use “All the news” dataset published by Thompson (2019). This dataset contains 204,135 news articles with headlines from 18 different American publications. It is an updated version of the dataset posted on Kaggle, containing over 50,000 more articles from a great number of publications. We use TorchText to preprocess our data. We define a field called TEXT for both the news articles and headlines. First, we define the TEXT field to be sequential and padded to a fixed length of 50. The articles and headlines are tokenized using spaCy and padded with a start token “SOS” and end token “EOS”. The final training dataset contains 170,000 pairs of articles and headlines, while the validation dataset contains 20,000. We then build the vocabulary and create the word embeddings using GloVE with dimension size of 100. Finally, the training and validation dataset are passed into BucketIterator to generate data loaders for PyTorch in batch size of 32 and sorted by the length of articles. The pipeline of our project is shown in the figure below: Project pipeline\" Project pipeline ","date":"2020-05-06","objectID":"/headline-writer/:0:3","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"Methods In this section we describe the methods we tried over the course of this project. First, we use (1) LSA as a non-deep learning baseline. For our deep learning baseline model, we start by implementing (2) sequence-to-sequence (Seq2Seq) model, a basic encoder-decoder model with RNNs, then improve our results by (3) adding attention mechanism and teacher forcing to the Seq2Seq model, and finally introduce (4) pointer-generator model with a coverage mechanism as an advanced deep learning approach. Summary of methods used in our experiments: Summary of methods used\" Summary of methods used LSA Latent semantic analysis (LSA) (Steinberger, 2014) uses singular value decomposition (SVD) on the term frequency-inverse document frequency (TF-IDF) matrix of a text input to extract the highest weighted sentence from the right singular matrix to be the output summary. This extractive summarization method is suitable for a non-deep learning baseline because it is efficient and requires very little computing resources. Baseline Seq2Seq Our basic Seq2Seq model is based on the Sequence to Sequence Learning with Neural Networks paper Sutskever et al (2014). This encoder-decoder model uses Recurrent Neural Network (RNNs) to encode the input text into a single vector, then decodes this vector by a second RNN, which learns to output the summary by generating one token at a time. At each time-step, the encoder RNN takes in the embedding of the current word $e(x_t)$, and the hidden state from the previous time-step $h_{t-1}$, and then outputs a new hidden state $h_t$. Once the final word is passed through the encoder, the decoder takes as input the hidden layers created by the encoder. The decoder has a similar structure as the encoder with an additional fully-connected layer after the output from each token to form each word of the generated headline. Each generated word is subsequently fed into the decoder as an input for generating the next word of the headline. Baseline Seq2Seq model\" Baseline Seq2Seq model In our experiment, we tested various RNN architectures; these include Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Bi-directional LSTM (BiLSTM) units, all which were chosen for their ability to handle sequential data. For our hyperparameters, we use only a single layer that contains 256 hidden units, with no dropouts (Lopyrev’s observations showed that dropout does not improve the model’s performance). We also use Cross Entropy as our loss function and Adam as our optimizer with a learning rate set to 0.001. These will be the default hyperparameters as we go forward. Attention Mechanism In our baseline Seq2Seq model, we pass the context vector to the decoder at every time-step and pass the context vector, embedded input word and hidden state to the linear layer to make a prediction. Although this reduces some information compression, the context vector still needs to contain all of the information about the source sentence. To solve this, we improve our previous model by introducing the additive attention mechanism from Bahdanau’s paper, which computes attention weights that allow the network to remember certain aspects of the input better. The attention distribution $a^t$ represents a probability distribution over the source words and is calculated as follows: $$e^t_i = v^T \\text{tanh}(W_h h_i + W_s s_t + b_{\\text{attention}})$$ $$a^t = \\text{softmax}(e^t)$$ In the equations above, $v$, $W_h$, $W_s$, $b_{\\text{attention}}$ are learnable parameters. The attention distribution is then used to calculate a weighted source vector (context vector) $h^*_t$ such that the weights sum to 1 and represent a weighted average over the last hidden layers after processing all of the input words: $$h^*_t = \\sum_{i=0} a^t_i h_i$$ The context vector can be viewed as a representation of what has been read from the source with a fixed size. This is computed at every time-step when decoding, which is concatenated with the decoder state $s_","date":"2020-05-06","objectID":"/headline-writer/:0:4","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"Results Quantitative Evaluation In our quantitative analysis, we use the BLEU evaluation metric, which calculates the fraction of the words (n-grams) in the machine generated summaries that appeared in the human reference summaries and also compares the length of the generated summary with the length of the true headline. This can be thought of as a measure of precision, and it is a popular metric used in NLP that is reported to have high correlation with human judgement, which is appropriate for our text summarization task. To reduce the computational burden, we first conducted all the experiments on a size of 10,000 article and headline pairs’ subset data, and the result is as shown below. The approaches we have tried can be roughly divided into 4 categories, including Non Deep Learning Base model, Deep Learning Base model, Improved Deep Learning model, and Advanced Deep Learning model. Result comparison on subset data with 10,000 rows\" Result comparison on subset data with 10,000 rows First and foremost, as for the base model, we implemented a linear LSA summarizer, and 3 base deep learning model using different types of RNN units, including LSTM, GRU, and BiLSTM. As for LSA summarizer, since it is a rule-based model, it does not have a loss. We can see that the BLEU score of LSA is merely 0.005, which means the model output doesn’t quite make sense. However, this is consistent with our intuition, because summarization is too complex a task for a linear model to achieve. As for deep learning models, BiLSTM is showing the best results of 0.015 BLEU score compared to the other two. Possible reason might be that it utilized the information not only from the previous context, but also the posterior context. Therefore, based on this, we decided to use BiLSTM as the base RNN unit to carry on with further experiments. Then, during the improvement part, we tried out 2 kinds of approaches, including teacher forcing and self-attention. It is clear to see that both teacher forcing and self-attention have managed to improve the BLEU score from 0.015 to 0.017, and 0.034 respectively, and together they can dramatically increase the BLEU to 0.045. This result can actually be credited to the advantage of self-attention and teacher forcing, because self-attention enables us to learn a probability distribution that tells us which word in the source text should be paid more attention to, which makes up for the flaw of BiLSTM that tends to forget the long-term dependencies. Besides, teacher forcing is also an effective training method to boost the performance by preventing the error from previous prediction being passed into the next state. Therefore, both of the methods are proven to be effective in our application scenario. In order to further improve our model performance in certain scenarios, (for example, to avoid generating faulty information), we implemented the pointer-generator and coverage structure on the basis of BiLSTM with self-attention as the advanced model (using teacher-forcing to train). Surprisingly, the loss and BLEU score after adding the pointer-generator and coverage is not as good as before. We think possible reasons might be that, on the one hand, we changed the loss function by adding coverage loss into it, which just increased component of loss function; on the other hand, although the metrics we used do not suggest improving, after doing qualitative analysis, we find the headline generation is actually improved. After the exploration phase, we retrained the models using the full dataset and the corresponding results are presented in the table below. It is not hard to find out that although the metrics are improved by introducing more data, the relative trend keeps the same, and our best model’s (BiLSTM with self-attention, pointer-generator, and coverage structure) BLEU score 0.075 actually beats our main source paper with BLEU of 0.010 (Lopyrev). Result comparison on full dataset\" Result comparison on full dataset","date":"2020-05-06","objectID":"/headline-writer/:0:5","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"Conclusion 1 - BiLSTM is the base RNN unit that has the best performance in our application scenario, (possible reason may be that BiLSTM can utilize the information from both previous and posterior context) 2 - Self-attention can help improve generator’s performance when dealing with long source article (by learning a probability distribution that tells us which part of the source text should be paid more attention to, which makes up for the flaw of BiLSTM that tends to forget the long-term dependencies) 3 - Teacher forcing is an effective training approach that can be used for Seq2Seq model scenario, (to prevent the error from previous prediction being passed into next state) 4 - Pointer-generator and coverage approach can significantly reduce the occurrence of the model generating faulty information by including the probability of directly outputting words from the source text 5 - Our best model (BiLSTM with self-attention, pointer-generator, and coverage structure) beats our main source paper’s BLEU score of 0.010 (Lopyrev) by 0.075 ","date":"2020-05-06","objectID":"/headline-writer/:0:6","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Natural Language Processing"],"content":"References Please see our paper for full list of references. ","date":"2020-05-06","objectID":"/headline-writer/:0:7","tags":null,"title":"Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network","uri":"/headline-writer/"},{"categories":["Computer Vision"],"content":"This repository contains implementations of Generative Adversarial Networks (GANs) to design and generate new face images of anime characters. A few improvement techniques were implemented to enhance the performance of the Deep Covolutional GANs (DCGANs) as well as the quality of the output. Also included is an implementation of Least Squares GAN (LSGAN) and NVIDIA’s open source StyleGAN. StyleGAN and StyleGAN2 are new state-of-the-art GAN architectures developed by a team from NVIDIA. These architectures address many of the shortcomings of previous GAN implementations and dramatically increase image quality. This work was done in collaboration with Riley Xu and Tianhao Lu. [paper] [code] ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:0","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Introduction Generative Adversarial Network (GAN) is a framework for deep learning models to generate superficial data mimicking a training distribution invented by Ian Goodfellow. Facebook’s AI research director Yann LeCun called adversarial training “the most interesting idea in the last 10 years of ML”. GANs have been successfully applied to many image categories, including objects as complicated as human faces, such as This Person Does Not Exist. There are many other attempts to apply GANs on paintings, objects, and even Pokemon. In this work, we train and generate images of character faces from Japanese anime. Anime is an incredibly popular media format around the world. However, coming up with new character designs takes tremendous effort and skill. GANs offer the opportunity to create new and custom designs without extensive input from a professional. This may help anime studios dramatically shorten development times, provide inspiration to professional and hobbyist character designers, and with sufficient progress may even be able to take over some aspects of the animation. ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:1","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Related Work There exist few attempts applying the GAN model to the problem of generating facial images of anime characters. DRAGAN gives a promising result (Jin, 2017); however, other attempts tend to be limited to blog posts and personal github repos, and the methodology and data samples are not well described by these sources. Moreover, the results of these projects are frequently defective, with effects such as asymmetric facial features and having large swaths that make them look like water-color paintings. Most disappointingly, these authors only generated female character faces. In this work, we seek to improve in the generation of anime character faces. In the following sections, we first describe the data set that we use. Then, we detail three GAN model architectures and show the results. Finally, we compare and quantify the differences between the architectures. ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:2","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Dataset Previous works mentioned above have datasets that suffer from high variance and noise, leading to results that were not too promising. Our goal for the data preparation is to find a clean, high-quality dataset and preprocess it in such a way that it can be used seamlessly with PyTorch. Our dataset consists of 21551 unlabeled anime faces obtained from Kaggle. This is a cleaner version of a dataset originally on Github where the images are fetched from Getchu and then cropped using the anime face detection algorithm (Nagadomi, 2011). The images are generally high-quality, but a few suffer from bad crops. Almost all images have white or otherwise minimal background. The range in art styles, pose, expressions, and hair is sufficiently variant. Notably, this data set does contain male characters, albeit at a smaller proportion, and our networks are trained to generate male faces unlike previous works. We transformed and loaded the dataset using Pytorch’s ImageFolder and DataLoader classes. We removed as many of the bad samples as possible, mainly poor crops. Additionally, all images are resized to 64x64 for further convenience. Sample images from dataset\" Sample images from dataset ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:3","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Generative Adversarial Network GAN uses a generator network $G$ to generate fake samples by mapping a latent space vector $z$ sampled from a normal distribution into a sample $G(z)$. GAN uses a min-max game where discriminator network $D$ tries to maximize its accuracy in classifying real and fake data and $G$ tries to minimize the probability that $D$ will predict its outputs are fake (Goodfellow, 2014). The objective can be formally expressed as: $$\\underset{\\scriptsize G}{\\text{\\scriptsize min}} \\underset{\\scriptsize D}{\\text{\\scriptsize max}}V(D,G) = \\mathbb{E}_x[\\log D(x)]\\space+\\space\\mathbb{E}_z[\\log(1-D(G(z)))]$$ The generator and discriminator train off of each other, enabling a GAN to learn unsupervised. In general, the training of a GAN occurs as below: 1 - Create a standard normally distributed vector $z$, with arbitrary feature length. 2 - Zero out gradient for $G$, generate fake image, and calculate $L_G$, where: $$L_G = \\frac{1}{n} \\sum_{i=0}^{n} L_{CE}(D(G(z)), 1)$$ 3 - Backpropogate $L_G$ and step up the optimizer 4 - Zero out gradient for $D$ and calculate $L_D$, where: $$L_D = \\frac{1}{2n} \\sum_{i=0}^{n} L_{CE}(D(X_i), 1) + L_{CE}(D(G(z)), 0)$$ 5 - Backpropogate $L_D$ and step up optimizer Overview of GAN process and data flow\" Overview of GAN process and data flow While all GANs have the same basic structure detailed above, there exist many subtleties that result in very different implementations. We explore three implementations: Deep Convolutional, Least Squares, and StyleGAN. ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:4","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Deep Convolutional GAN (DCGAN) Our base model architecture is inspired by Deep Convolutional GAN (DCGAN) (Radford, 2015). The discriminator is made up of strided convolution layers, 2d batch norm layers, LeakyReLUs, and outputs the final probability through a Sigmoid activation function. The generator consists of a series of strided two dimensional convolutional transpose layers, each followed by a 2d batch norm layer and a ReLU activation. The outputs go through a tanh function. Generator architecture - it projects latent vector $z$ to a 64x64 RBG image through a series of fractionally-strided convolutions\" Generator architecture - it projects latent vector $z$ to a 64x64 RBG image through a series of fractionally-strided convolutions Training and Hyperparameter Tuning The figures below show $L_G$ and $L_D$ throughout our training: Generator loss\" Generator loss Discriminator loss\" Discriminator loss Some things we watched out for during training: 1 - We want to prevent the $L_D$ from going to 0 (this represents a failure mode of training). 2 - Both $L_D$ and $L_G$ should not decrease monotonically (they should oscillate around a certain loss), with $L_G$ increasing as a whole and $L_D$ decreasing as a whole. 3 - Display sample generated images every a certain number of iterations to watch for mode collapse. We used a batch size of 64 and a feature length of 100 for vector $z$. For both the $G$ and $D$, we use Adam optimizer because it combines the best properties of the AdaGrad and RMSProp algorithms, which can handle sparse gradients on noisy problems. Since $D$ learns much faster than $G$, we set the learning rate for optimizer for $G$ at 0.0005 and optimizer for $D$ at 0.0001. Additionally, we used fuzzy labels - instead of a hard 0 or 1 as labels for the discriminator, we set them to 0.1 and 0.9 to make the discriminator weaker. Progress of DCGAN generator\" Progress of DCGAN generator Results Uncurated sample images from our DCGAN model are shown in the figure below. The model was trained on 150 epochs. We find these results quite promising, because most of the generated images are convincing. However, some mistakes are made by the model, such as twisted and broken faces, asymmetric eyes, and non-uniformity in hair. The flaws urge us to try new models. DCGAN generated samples\" DCGAN generated samples ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:5","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Least Squares GAN (LSGAN) A large problem with DCGAN is that it can suffer from vanishing gradients. A variation called Least Squares GAN (LSGAN) attempts to combat the vanishing gradients by using a loss function that provides smooth and non-saturating gradient in discriminator $D$ (Mao, 2016). Formally, the objective functions can be expressed as such: $$\\underset{D}{\\text{min}}V_{LS}(D) = \\frac{1}{2}\\mathbb{E}_{x}[(D(x) - 1)^2] + \\frac{1}{2}\\mathbb{E}_{z}[(D(G(z)))^2]$$ $$\\underset{G}{\\text{min}}V_{LS}(G) = \\frac{1}{2} \\mathbb{E}_{z}[(D(G(z)) - 1)^2]$$ Training and Hyperparameter Tuning The model architecture we used for LSGAN is exactly the same as that of DCGAN, with a few minor changes. First, we removed the sigmoid activation function at the end of $D$. Moreover, we adjusted the $L_G$ and $L_D$ functions to follow the objective functions described above. We also use the same hyperparameters as we did for DCGAN. Results Uncurated samples are shown in the figure below, trained on 150 epochs. As expected, LSGAN performs more stable during the learning process. Unfortunately, our generated samples appear to have more artifacts and less variation. LSGAN generated samples\" LSGAN generated samples ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:6","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"StyleGAN StyleGAN and StyleGAN2 are new state-of-the-art GAN architectures developed by a team from NVIDIA (Karras, 2020). These architectures address many of the shortcomings of previous GAN implementations and dramatically increase image quality. StyleGAN architecture\" StyleGAN architecture StyleGAN uses a progressive architecture. Both the generator and discriminator are first trained only on 4x4 pixel images. After sufficient progress, a new layer block is added to both networks to handle 8x8 images. This doubling procedure continues until reaching the desired image size. This progression allows the generator to learn fine features after already having learned coarse features, significantly easing the training. Another key innovation is the adoption of techniques from style-transfer learning. Instead of inputting the latent vector directly into the first layer of the convolution, there is a separate mapping network that processes the latent vector into an intermediate latent vector. This is then transformed into a style vector $y_i$, via a learned affine transformation, and fed into the synthesis network between each convolution layer. The style transfer is done via adaptive instance normalization (AdaIN): $$\\text{AdaIN}(x, y_i) = y_{i,s} \\frac{x - \\bar x}{\\sigma(x)} + y_{i,b}$$ Above, $y_i = (y_{i,s}, y_{i,b})$ is the style scale and bias in layer $i$, and $x$ is the input activations from the previous layer. Results Using a baseline StyleGAN architecture implemented in PyTorch, we achieved remarkable results after 150 epochs of training. Uncurated sample fakes are shown in the figure below. Artifacts rarely appear, and there is huge variation between samples. By eye we are generally unable to distinguish fake from real images. StyleGAN generated samples\" StyleGAN generated samples ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:7","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Quantifying Results A common metric to quantify the quality of generated images is the Frechet Inception Distance (FID) (Heusel, 2017). FID is an improvement on the Inception Score (Salimans, 2016) and calculates the similarity between two sets of images, unsupervised. As with Inception Score, FID uses the Inception V3 model, a trained classifier on 1000 objects. However, instead of using the output of the Inception V3 model, FID calculates the Frechet distance of the activations in the coding layer. Because classifier networks tend to learn general features, using the coding layer generalizes the Inception Score to arbitrary image sets with good effect. We calculated the FIDs comparing generated images from each GAN implementation with the original data set. All implementations were trained on 150 epochs. The results are summarized in the table below; note lower FID scores are better. StyleGAN outperformed DCGAN, while LSGAN seriously underperformed. These observations corroborate our qualitative assessments. FID calculations for various GAN implementations\" FID calculations for various GAN implementations ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:8","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Conclusion In this work, we explored the artificial creation of the anime characters using GANs. By extracting a clean data set and introducing several practical training strategies, we showed that DCGANs can produce convincing fakes. We then implemented StyleGAN, a major improvement on DCGAN both qualitatively and by FID score. Additionally, StyleGAN can be trivially used to mix styles. With the power of style mixing, many possibilities exist for future developments. Assuming a data set with labels, the model can learn to generate specific features on-demand, such as hair color or pose. Other directions are to improve the final resolution of generated images, or to generate full-body designs. GANs are a very new innovation, but it will not be long before they become used in mainstream disciplines. This work has demonstrated that GANs can be readily applied to the anime industry, allowing for streamlined design of new characters. ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:9","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"Acknowledgement We acknowledge NVIDIA for their open-source StyleGAN implementation, Google for supplying the Inception V3 model, and Github user mseitzer for creating a PyTorch implementation of the FID calculation. ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:10","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":["Computer Vision"],"content":"References Please see our paper for full list of references. ","date":"2020-05-05","objectID":"/artificial-anime-character-design/:0:11","tags":null,"title":"Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs)","uri":"/artificial-anime-character-design/"},{"categories":null,"content":" -- -- Roy Wu M.S. in Data Science University of Pennsylvania Philadelphia, PA Riverside, CA wuroy@seas.upenn.edu +1 (973) 901-1028 [view resume] SKILLS Programming Python SQL Scala Java Libraries PyTorch Scikit-Learn NumPy Pandas Matplotlib Seaborn Technologies Jupyter Apache Spark AWS Git Docker LANGUAGESEnglish(Native) Chinese, Mandarin(Fluent) ABOUT MEHi! My name is Roy and I am a Data Scientist interested in generative models. I am currently pursuing a M.S. in Data Science at the University of Pennsylvania, with a concentration in Computer Vision. During Summer 2020, I am working at The Trade Desk (NASDAQ: TTD) as a Data Science Intern. I have 2 years of experience working in the computer software and FinTech industry as a Solutions Analyst. I also have a B.A. in Mathematics and Economics from New York University. I’m a data enthusiast with a strong background in statistics and extensive hands-on experience in applying machine learning and big data tools including Python, SQL, Scala, and Spark. My hobbies include piano, swimming, muay thai, skateboarding, and filming \u0026 video editing, particulary making timelapses and hyperlapses. WORK EXPERIENCEThe Trade Desk Data Science Intern Jun 2020 - Present  | Boulder, CO Performance automation team Vitech Systems Group Solutions Analyst Jun 2017 - Jul 2019  | New York, NY Led cross-functional team collaboration and served as liaison between engineering team and clients on agile-based development projects Haver Analytics Economic Research Intern Sep 2016 - May 2017  | New York, NY Compiled, managed, and quality assured databases covering macroeconomic indicators of Asia-Pacific markets and automated extraction/formatting of data PrimeAlpha Project Manager Intern Oct 2014 - Feb 2015  | New York, NY Managed various ad hoc projects, such as conducting investor due diligence and developing marketing campaigns for product launching EDUCATIONUniversity of Pennsylvania School of Engineering \u0026 Applied Science Expected May 2021  | Philadelphia, PA Master of Science in Engineering in Data Science New York University College of Arts \u0026 Science Aug 2013 - May 2017  | New York, NY Bachelor of Arts in Mathematics and Economics   ","date":"2020-05-24","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":" -- Categories:   Computer Vision   Natural Language Processing May 2020 Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network Roy Wu, Henglin Wu, Ruilin Zhao, Chenyuan Li This repository contains implementations of Sequence-to-sequence (Seq2Seq) neural networks for abstractive text summarization. We implement Attention mechanism, Teacher Forcing algorithm, and Pointer-Generator Network (inspired by Get To The Point: Summarization with Pointer-Generator Networks) in our experiment to improve our baseline models. Natural Language Processing blog paper code May 2020 Artificial Anime Character Design: An Application of Generative Adversarial Networks (GANs) Roy Wu, Riley Xu, Tianhao Lu This repository contains implementations of Generative Adversarial Networks (GANs) to design and generate new face images of anime characters. A few improvement techniques were implemented to enhance the performance of the Deep Covolutional GANs (DCGANs) as well as the quality of the output. Also included is an implementation of Least Squares GAN (LSGAN) and NVIDIA's open source StyleGAN. Computer Vision blog paper code ","date":"2020-05-24","objectID":"/projects/:0:0","tags":null,"title":"Projects","uri":"/projects/"}]