<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Natural Language Processing - Category - Roy Wu</title>
        <link>https://roynwu.github.io/categories/natural-language-processing/</link>
        <description>Natural Language Processing - Category - Roy Wu</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 06 May 2020 23:16:22 -0700</lastBuildDate><atom:link href="https://roynwu.github.io/categories/natural-language-processing/" rel="self" type="application/rss+xml" /><item>
    <title>Headline-Writer: Abstractive Text Summarization with Attention and Pointer-Generator Network</title>
    <link>https://roynwu.github.io/headline-writer/</link>
    <pubDate>Wed, 06 May 2020 23:16:22 -0700</pubDate>
    <author>Author</author>
    <guid>https://roynwu.github.io/headline-writer/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/img/projects/headline-writer/featured-image.JPG" referrerpolicy="no-referrer">
            </div>This repository contains implementations of Sequence-to-sequence (Seq2Seq) neural networks for abstractive text summarization. We implement Attention mechanism, Teacher Forcing algorithm, and Pointer-Generator Network (inspired by Get To The Point: Summarization with Pointer-Generator Networks) in our experiment to improve our baseline models. This work was done in collaboration with Henglin Wu, Ruilin Zhao, and Chenyuan Li.
[paper] [code]
Watch our video presentation below:
  Introduction Sequence-to-sequence (Seq2Seq) neural networks have been proved to be effective in tasks that involve transforming text from one form to another (e.]]></description>
</item></channel>
</rss>
